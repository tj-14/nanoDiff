{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:00:10.845784Z",
     "start_time": "2023-09-07T07:00:07.969752Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.training_utils import EMAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:00:10.913045Z",
     "start_time": "2023-09-07T07:00:10.786677Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingArgs:\n",
    "    image_size = 512  # the generated image resolution\n",
    "    train_batch_size = 1\n",
    "    num_train_epochs = 1000\n",
    "    max_train_steps = 1000\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    mixed_precision = \"fp16\"\n",
    "    weight_dtype = torch.float16\n",
    "    output_dir = \"ddpm-pokemon-64\"\n",
    "    data_dir = Path(\"../dataset/pokemon/Generation_1\") \n",
    "    \n",
    "    pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "    use_ema = False\n",
    "    guidance_scale = 7.5\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "    adam_weight_decay = 1e-2\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1\n",
    "\n",
    "    resume_from_checkpoint = \"latest\"\n",
    "    validation_prompts = [\"a pokemon with a fire tail\"]\n",
    "    validation_epochs = 100\n",
    "    logging_dir = \"ddpm-pokemon-64/logs\"\n",
    "    report_to = \"tensorboard\"\n",
    "    seed = 0\n",
    "\n",
    "args = TrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:00:10.913294Z",
     "start_time": "2023-09-07T07:00:10.795025Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "accelerator_project_config = ProjectConfiguration(\n",
    "    project_dir=args.output_dir, logging_dir=args.logging_dir\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:00:10.923714Z",
     "start_time": "2023-09-07T07:00:10.811501Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f26811dbdd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_model_hook(models, weights, output_dir):\n",
    "    if args.use_ema:\n",
    "        ema_unet.save_pretrained(os.path.join(output_dir, \"unet_ema\"))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model.save_pretrained(os.path.join(output_dir, \"unet\"))\n",
    "\n",
    "        # make sure to pop weight so that corresponding model is not saved again\n",
    "        weights.pop()\n",
    "\n",
    "def load_model_hook(models, input_dir):\n",
    "    if args.use_ema:\n",
    "        load_model = EMAModel.from_pretrained(\n",
    "            os.path.join(input_dir, \"unet_ema\"), UNet2DConditionModel\n",
    "        )\n",
    "        ema_unet.load_state_dict(load_model.state_dict())\n",
    "        ema_unet.to(accelerator.device)\n",
    "        del load_model\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        # pop models so that they are not loaded again\n",
    "        model = models.pop()\n",
    "\n",
    "        # load diffusers style into model\n",
    "        load_model = UNet2DConditionModel.from_pretrained(\n",
    "            input_dir, subfolder=\"unet\"\n",
    "        )\n",
    "        model.register_to_config(**load_model.config)\n",
    "\n",
    "        model.load_state_dict(load_model.state_dict())\n",
    "        del load_model\n",
    "\n",
    "accelerator.register_save_state_pre_hook(save_model_hook)\n",
    "accelerator.register_load_state_pre_hook(load_model_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:00:10.923894Z",
     "start_time": "2023-09-07T07:00:10.832299Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "set_seed(args.seed)\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{args.output_dir}/png\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:00:16.080829Z",
     "start_time": "2023-09-07T07:00:11.232624Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    subfolder=\"text_encoder\",\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    subfolder=\"unet\",\n",
    ")\n",
    "\n",
    "# Freeze vae and text_encoder\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "if args.use_ema:\n",
    "    ema_unet = UNet2DConditionModel.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"unet\"\n",
    "    )\n",
    "\n",
    "    ema_unet = EMAModel(\n",
    "        ema_unet.parameters(),\n",
    "        model_cls=UNet2DConditionModel,\n",
    "        model_config=ema_unet.config,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:15.050668Z",
     "start_time": "2023-09-07T07:01:15.042780Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_captions(captions, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        captions,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:15.252126Z",
     "start_time": "2023-09-07T07:01:15.241295Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((args.image_size, args.image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:15.546935Z",
     "start_time": "2023-09-07T07:01:15.419117Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_dict = {\"image\": [], \"text\": []}\n",
    "for file in os.listdir(args.data_dir):\n",
    "    dataset_dict[\"image\"].append(Image.open(args.data_dir / file))\n",
    "    dataset_dict[\"text\"].append(file.split(\"_\")[0])\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:16.433215Z",
     "start_time": "2023-09-07T07:01:15.681770Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples[\"image\"]]\n",
    "    examples[\"pixel_values\"] = [preprocess(image) for image in images]\n",
    "    examples[\"input_ids\"] = [tokenize_captions(caption, tokenizer) for caption in examples[\"text\"]]\n",
    "    return examples\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:16.470338Z",
     "start_time": "2023-09-07T07:01:16.397341Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:16.547440Z",
     "start_time": "2023-09-07T07:01:16.470542Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:17.074952Z",
     "start_time": "2023-09-07T07:01:17.065531Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode_noise(noise, vae):\n",
    "    with torch.no_grad():\n",
    "        return vae.decode(noise / vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "def denormalize(images):\n",
    "    \"\"\"\n",
    "    Denormalize an image array to [0,1].\n",
    "    \"\"\"\n",
    "    return (images / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "def numpy_to_pil(image):\n",
    "    return Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "\n",
    "def noise_to_pil(noise, vae):\n",
    "    return numpy_to_pil(denormalize(decode_noise(noise, vae)).cpu().permute(0, 2, 3, 1)[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:18.118010Z",
     "start_time": "2023-09-07T07:01:18.080343Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unet.enable_gradient_checkpointing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:18.422035Z",
     "start_time": "2023-09-07T07:01:18.399320Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    unet.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    eps=args.adam_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T07:01:23.321401Z",
     "start_time": "2023-09-07T07:01:20.627881Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "text_encoder, vae, unet, optimizer, train_dataloader = accelerator.prepare(\n",
    "    text_encoder, vae, unet, optimizer, train_dataloader\n",
    ")\n",
    "if args.use_ema:\n",
    "    ema_unet.to(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:57:41.896701Z",
     "start_time": "2023-09-07T06:57:41.889937Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(prompt, tokenizer, text_encoder, unet, scheduler, device):\n",
    "    text_input_ids = tokenize_captions([\"\", prompt], tokenizer)\n",
    "\n",
    "    prompt_embeds = text_encoder(text_input_ids.squeeze(1))\n",
    "    prompt_embeds = prompt_embeds[0]\n",
    "\n",
    "    noise = torch.randn(\n",
    "        (1, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(scheduler.timesteps)\n",
    "        for t in progress_bar:\n",
    "            noise_input = torch.cat([noise] * 2)\n",
    "\n",
    "            model_output = unet(\n",
    "                noise_input,\n",
    "                t,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "            )[0]\n",
    "            noise_pred_uncond, noise_pred_text = model_output.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + args.guidance_scale * (\n",
    "                    noise_pred_text - noise_pred_uncond\n",
    "            )\n",
    "\n",
    "            noise = scheduler.step(noise_pred, t, noise)[0]\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T06:57:44.548338Z",
     "start_time": "2023-09-07T06:57:41.904763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 'latest' does not exist. Starting a new training run.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45b41f8be6e41a09a99855617fa9c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.74 GiB total capacity; 14.19 GiB already allocated; 39.69 MiB free; 14.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 115\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39msync_gradients:\n\u001b[1;32m    114\u001b[0m         accelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(unet\u001b[38;5;241m.\u001b[39mparameters(), args\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[0;32m--> 115\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Checks if the accelerator has performed an optimization step behind the scenes\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/accelerate/optimizer.py:132\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:374\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 374\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:290\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 290\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/accelerate/optimizer.py:185\u001b[0m, in \u001b[0;36mpatch_optimizer_step.<locals>.patched_step\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatched_step\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    184\u001b[0m     accelerated_optimizer\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/torch/optim/adamw.py:564\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    562\u001b[0m     denom \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_add(max_exp_avg_sq_sqrt, eps)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    566\u001b[0m     denom \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_add(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.74 GiB total capacity; 14.19 GiB already allocated; 39.69 MiB free; 14.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "accelerator.init_trackers(args.output_dir)\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "text_encoder.to(accelerator.device, dtype=args.weight_dtype)\n",
    "vae.to(accelerator.device, dtype=args.weight_dtype)\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    if args.resume_from_checkpoint != \"latest\":\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = os.listdir(args.output_dir)\n",
    "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "    if path is None:\n",
    "        accelerator.print(\n",
    "            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "        )\n",
    "        args.resume_from_checkpoint = None\n",
    "    else:\n",
    "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "        accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "        global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "        resume_global_step = global_step * args.gradient_accumulation_steps\n",
    "        first_epoch = global_step // num_update_steps_per_epoch\n",
    "        resume_step = resume_global_step % (\n",
    "            num_update_steps_per_epoch * args.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(\n",
    "    range(global_step, args.max_train_steps),\n",
    "    disable=not accelerator.is_local_main_process,\n",
    ")\n",
    "progress_bar.set_description(\"Steps\")\n",
    "\n",
    "for epoch in range(first_epoch, args.num_train_epochs):\n",
    "    unet.train()\n",
    "    train_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Skip steps until we reach the resumed step\n",
    "        if (\n",
    "            args.resume_from_checkpoint\n",
    "            and epoch == first_epoch\n",
    "            and step < resume_step\n",
    "        ):\n",
    "            if step % args.gradient_accumulation_steps == 0:\n",
    "                progress_bar.update(1)\n",
    "            continue\n",
    "\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Convert images to latent space\n",
    "            latents = vae.encode(\n",
    "                batch[\"pixel_values\"].to(args.weight_dtype)\n",
    "            ).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                noise_scheduler.config.num_train_timesteps,\n",
    "                (bsz,),\n",
    "                device=latents.device,\n",
    "            )\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"].squeeze(1))[0]\n",
    "\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown prediction type {noise_scheduler.config.prediction_type}\"\n",
    "                )\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            model_pred = unet(\n",
    "                noisy_latents, timesteps, encoder_hidden_states\n",
    "            ).sample\n",
    "\n",
    "            loss = F.mse_loss(\n",
    "                model_pred.float(), target.float(), reduction=\"mean\"\n",
    "            )\n",
    "\n",
    "            # Gather the losses across all processes for logging (if we use distributed training).\n",
    "            avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
    "            train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
    "\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            if args.use_ema:\n",
    "                ema_unet.step(unet.parameters())\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "            train_loss = 0.0\n",
    "\n",
    "            if global_step % args.checkpointing_steps == 0:\n",
    "                if accelerator.is_main_process:\n",
    "                    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "                    if args.checkpoints_total_limit is not None:\n",
    "                        checkpoints = os.listdir(args.output_dir)\n",
    "                        checkpoints = [\n",
    "                            d for d in checkpoints if d.startswith(\"checkpoint\")\n",
    "                        ]\n",
    "                        checkpoints = sorted(\n",
    "                            checkpoints, key=lambda x: int(x.split(\"-\")[1])\n",
    "                        )\n",
    "\n",
    "                        # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
    "                        if len(checkpoints) >= args.checkpoints_total_limit:\n",
    "                            num_to_remove = (\n",
    "                                len(checkpoints) - args.checkpoints_total_limit + 1\n",
    "                            )\n",
    "                            removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                            logger.info(\n",
    "                                f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n",
    "                            )\n",
    "                            logger.info(\n",
    "                                f\"removing checkpoints: {', '.join(removing_checkpoints)}\"\n",
    "                            )\n",
    "\n",
    "                            for removing_checkpoint in removing_checkpoints:\n",
    "                                removing_checkpoint = os.path.join(\n",
    "                                    args.output_dir, removing_checkpoint\n",
    "                                )\n",
    "                                shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                    save_path = os.path.join(\n",
    "                        args.output_dir, f\"checkpoint-{global_step}\"\n",
    "                    )\n",
    "                    accelerator.save_state(save_path)\n",
    "                    logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "        logs = {\n",
    "            \"step_loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    if (\n",
    "        args.validation_prompts is not None\n",
    "        and epoch % args.validation_epochs == 0\n",
    "    ):\n",
    "        if args.use_ema:\n",
    "            # Store the UNet parameters temporarily and load the EMA parameters to perform inference.\n",
    "            ema_unet.store(unet.parameters())\n",
    "            ema_unet.copy_to(unet.parameters())\n",
    "        \n",
    "        noise = inference(\n",
    "            args.validation_prompts[0],\n",
    "            tokenizer,\n",
    "            text_encoder,\n",
    "            unet,\n",
    "            noise_scheduler,\n",
    "            accelerator.device,\n",
    "        )\n",
    "        im = noise_to_pil(noise)\n",
    "        im.save(f\"{args.output_dir}/png/{epoch}.png\")\n",
    "\n",
    "        if args.use_ema:\n",
    "            # Switch back to the original UNet parameters.\n",
    "            ema_unet.restore(unet.parameters())\n",
    "\n",
    "unet = accelerator.unwrap_model(unet)\n",
    "if args.use_ema:\n",
    "    ema_unet.copy_to(unet.parameters())\n",
    "torch.save(unet.state_dict(), f\"{args.output_dir}/unet.pth\")\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
